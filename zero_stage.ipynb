{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "from lxml import html\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from lxml.etree import ParseError\n",
    "from lxml.etree import ParserError\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from aiohttp import ClientTimeout\n",
    "from aiohttp_socks import ProxyConnector\n",
    "from aiohttp import ClientSession\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "\n",
    "from tqdm.asyncio import tqdm\n",
    "from charset_normalizer import from_bytes\n",
    "\n",
    "from datetime import datetime as Ddtime\n",
    "from datetime import timedelta\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST_AMOUNT = 10 # больше 10 - плохая идея\n",
    "\n",
    "semaphore = asyncio.Semaphore(REQUEST_AMOUNT)\n",
    "\n",
    "### Случайные конфигурации юзера и языков запрашиваемых страниц\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36 Edg/93.0.961.52\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36\",\n",
    "]\n",
    "\n",
    "LANGUAGES = [\n",
    "    \"en-US,en;q=0.9\",\n",
    "    \"en-GB,en;q=0.9\",\n",
    "    \"fr-FR,fr;q=0.9\",\n",
    "    \"de-DE,de;q=0.9\",\n",
    "    \"es-ES,es;q=0.9\",\n",
    "    \"ru-RU,ru;q=0.9\",\n",
    "    \"zh-CN,zh;q=0.9\",\n",
    "    \"ja-JP,ja;q=0.9\",\n",
    "    \"it-IT,it;q=0.9\",\n",
    "    \"pt-BR,pt;q=0.9\",\n",
    "]\n",
    "\n",
    "ip_change_lock = asyncio.Lock()\n",
    "\n",
    "TOR_PROXY = \"socks5://127.0.0.1:9150\"  # Порт Tor Browser\n",
    "CONTROL_PORT = 9051  # Порт ControlPort\n",
    "CONTROL_PASSWORD = \"ENTER_YOUR_OWN_PASSWORD\"\n",
    "\n",
    "### Торовский прокси-сервер\n",
    "proxies = {\n",
    "    'http': 'socks5://127.0.0.1:9150',\n",
    "    'https': 'socks5://127.0.0.1:9150'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Функция для обновления IP внутри Тора (используется для\n",
    "### преодоления 429 ответа от сайтов и прочих неприятностей)\n",
    "def renew_tor_ip():\n",
    "    with Controller.from_port(port=CONTROL_PORT) as controller:\n",
    "        controller.authenticate(password=CONTROL_PASSWORD)\n",
    "        controller.signal(Signal.NEWNYM)\n",
    "\n",
    "### Основная функция для асинхронного скачивания HTML-кода страницы\n",
    "### запрашиваемого ресурса с внедренным обработчиком ошибок\n",
    "async def fetch(session, url, max_retries=10):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            async with semaphore:\n",
    "                await asyncio.sleep(1)  # Задержка между запросами\n",
    "                headers = {\n",
    "                    \"User-Agent\": random.choice(USER_AGENTS),\n",
    "                    \"Accept-Language\": random.choice(LANGUAGES),\n",
    "                }\n",
    "                async with session.get(url, headers=headers) as response:\n",
    "                    if response.status == 200 or response.status == 404:\n",
    "                        content = await response.content.read()\n",
    "                        en = from_bytes(content)\n",
    "                        encoding = en.best().encoding\n",
    "                        return content.decode(encoding)\n",
    "                    elif response.status == 429:  # Too Many Requests\n",
    "                        retry_after = response.headers.get(\"Retry-After\")\n",
    "                        if retry_after:\n",
    "                            try:\n",
    "                                wait_time = int(retry_after)\n",
    "                            except ValueError:\n",
    "                                retry_time = datetime.strptime(retry_after, \"%a, %d %b %Y %H:%M:%S GMT\")\n",
    "                                wait_time = (retry_time - datetime.now()).total_seconds()\n",
    "                            await asyncio.sleep(wait_time)\n",
    "                        else:\n",
    "                            async with ip_change_lock:\n",
    "                                renew_tor_ip()\n",
    "                                await asyncio.sleep(15)\n",
    "                    else:\n",
    "                        async with ip_change_lock:\n",
    "                            print(f\"Server returned {response.status} for {url}. Changing IP...\")\n",
    "                            renew_tor_ip()\n",
    "                            await asyncio.sleep(15)\n",
    "                    retries += 1\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                async with ip_change_lock:\n",
    "                    print(\"Changing IP due to error...\")\n",
    "                    renew_tor_ip()\n",
    "                    await asyncio.sleep(15)\n",
    "    return None\n",
    "\n",
    "### Сверка того, является ли извлеченная ссылка\n",
    "### на модель автомобиля корректно сгенерированной или является\n",
    "### \"синонимом\" для корректной (насколько я понимаю логику дрома)\n",
    "async def process_page(url, session):\n",
    "    content = await fetch(session, url)\n",
    "    if content:\n",
    "        tree = html.fromstring(content)\n",
    "        title = tree.xpath('//head/title')[0]\n",
    "        return title.text_content() != \"Запрошенная вами страница не существует!\"\n",
    "    return None\n",
    "\n",
    "### Базовый обработчик для асинхронного запроса\n",
    "async def worker(url_queue, session, results, pbar):\n",
    "    while True:\n",
    "        url = await url_queue.get()\n",
    "        if url is None:\n",
    "            break\n",
    "        try:\n",
    "            data = await process_page(url, session)\n",
    "            if data and type(data) != type(None):\n",
    "                results.append(url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "        finally:\n",
    "            url_queue.task_done()\n",
    "            pbar.update(1)\n",
    "\n",
    "### Функция для обраюботки корректных ссылок, извлеченных из скрипта\n",
    "### основной страницы для машины на сайте Дром\n",
    "async def extracting_models_url(urls):\n",
    "    url_queue = asyncio.Queue()\n",
    "    results = []\n",
    "\n",
    "    for url in urls:\n",
    "        url_queue.put_nowait(url)\n",
    "\n",
    "    connector = ProxyConnector.from_url(TOR_PROXY)\n",
    "    async with ClientSession(connector=connector) as session:\n",
    "        with tqdm(total=len(urls), desc=f\"Сбор корректных ссылок\") as pbar:\n",
    "            workers = [asyncio.create_task(worker(url_queue, session, results, pbar)) for _ in range(REQUEST_AMOUNT)]\n",
    "            \n",
    "            await url_queue.join()\n",
    "\n",
    "            for _ in range(REQUEST_AMOUNT):\n",
    "                await url_queue.put(None)\n",
    "\n",
    "            await asyncio.gather(*workers)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Базовая функция для подсчета количества объявлений\n",
    "### на запрашиваемой странице. Если таковых там не оказалось\n",
    "### (а такое бывает, потому что Дром работает как большая \n",
    "### таблица из SQL), возвращает нарочито большое значение 10**8\n",
    "async def adding_amount(url):\n",
    "    connector = ProxyConnector.from_url(TOR_PROXY)\n",
    "    async with ClientSession(connector=connector) as session:\n",
    "        content = await fetch(session, url)\n",
    "        if content:\n",
    "            tree = html.fromstring(content)\n",
    "            if len(tree.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')) == 0:\n",
    "                return 10**8\n",
    "            amo = html.tostring(tree.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')[0]).decode(\"utf-8\")\n",
    "            amo = amo[amo.find(\">\") + 1:].split()[0].replace(\"&#160;\", \"\")\n",
    "            return int(amo)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNumber(s):\n",
    "    negative = False\n",
    "    if(s[0] =='-'):\n",
    "        negative = True\n",
    "         \n",
    "    if negative == True:\n",
    "        s = s[1:]\n",
    "    try:\n",
    "        n = int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Функция-экстрактор моделей автомобиля из сырого HTML-кода, который\n",
    "### на сайте генерирует отдельный .js файлик\n",
    "def getting_models_for_car(car_name):\n",
    "    url = f'https://auto.drom.ru/archive/{car_name}/all/'\n",
    "    response = requests.get(url, proxies = proxies)\n",
    "    tree = html.fromstring(response.content)\n",
    "\n",
    "    script_to_read = tree.xpath('//script[@data-drom-module=\"bulls-list-auto\"]')[0]\n",
    "\n",
    "    script_to_read = html.tostring(script_to_read).decode(\"utf-8\")\n",
    "\n",
    "    quot = \"&quot;\"*(script_to_read.find(\"&quot;\") != -1) + '\"'*(script_to_read.find(\"&quot;\") == -1)\n",
    "    fltr = script_to_read.find(quot + \"filter\" + quot + \":\")\n",
    "\n",
    "    script_to_read = script_to_read[fltr:]\n",
    "    ending = script_to_read.find(\"],\" + quot + \"firmId\")\n",
    "    script_to_read = script_to_read[:ending]\n",
    "    script_to_read = script_to_read[script_to_read.find(\"[{\") + 1 : script_to_read.rfind(\"]\")]\n",
    "    script_to_read = script_to_read.split(\"},{\")\n",
    "    script_to_read = [x.replace(quot, \" \") for x in script_to_read]\n",
    "    script_to_read = [x[x.rfind(\"[\"):] for x in script_to_read]\n",
    "\n",
    "    all_models = []\n",
    "\n",
    "    for model in (script_to_read):\n",
    "        probab_list = model[1:-1].split(\", \")\n",
    "        probab_list = [x.strip() for x in probab_list]\n",
    "        for cd in probab_list:\n",
    "            if cd.find(\"\\\\\") == -1 and cd.find(\" \") == -1:\n",
    "                all_models.append(cd)\n",
    "\n",
    "    all_models = [f'https://auto.drom.ru/archive/{car_name}/{x.replace(\" \", \"\")}/' for x in all_models]\n",
    "    \n",
    "    return all_models\n",
    "\n",
    "### Проверка корректности страницы (рудиментарная функция)\n",
    "def url_is_ok(url):\n",
    "    response = requests.get(url, proxies = proxies)\n",
    "    tree = html.fromstring(response.content)\n",
    "    title = tree.xpath('//head/title')[0]\n",
    "    return title.text_content() != \"Запрошенная вами страница не существует!\"\n",
    "\n",
    "### Проверка корректности страницы (рудиментарная функция)\n",
    "def url_is_valid(url):\n",
    "    response = requests.get(url, proxies = proxies)\n",
    "    tree = html.fromstring(response.content)\n",
    "    divs = tree.xpath('//div[@data-ftid=\"bulls-list_bull\"]')\n",
    "\n",
    "    return len(divs) > 0\n",
    "\n",
    "### Функция-экстрактор всех доступных поколений и рестайлингов\n",
    "### конкретной модели автомобиля. Много эвристик, о которых ниже\n",
    "async def getting_spec_models_for_car(car_url, driver):\n",
    "    mas = [0, 0]\n",
    "    generation = 1\n",
    "    new_url = car_url + f\"generation{generation}/restyling{mas[generation]}/\"\n",
    "\n",
    "    all_urls = []\n",
    "    amo = await adding_amount(car_url)\n",
    "    if amo >= 10**8:\n",
    "        return all_urls\n",
    "\n",
    "    schet = 0\n",
    "\n",
    "    ### Основная рудиментарная эвристика: перебирать последовательно\n",
    "    ### поколения и рестайлинги конкретной модели \"пока перебирается\"\n",
    "    ### Проблемы: отсутствие асинхронности, медлительность и не гарантированность\n",
    "    ### результата. Для этого ниже есть более продуманный блок кода\n",
    "    while url_is_ok(new_url):\n",
    "        schet += 1\n",
    "        while url_is_ok(new_url):\n",
    "            mas[generation] += 1\n",
    "            new_url = car_url + f\"generation{generation}/restyling{mas[generation]}/\"\n",
    "        generation += 1\n",
    "        mas.append(0)\n",
    "        new_url = car_url + f\"generation{generation}/restyling{mas[generation]}/\"\n",
    "    \n",
    "    ### Если ничего не сработало в прошлом пункте (почти всегда),\n",
    "    ### открываем в эмуляторе саму страницу, находим кнопку выбора модели,\n",
    "    ### нажимаем, и это нам дает возможность скачать опять сырой код страницы,\n",
    "    ### где уже будут все модели и все их рестайлинги\n",
    "    if schet <= 1:\n",
    "        driver.get(car_url)\n",
    "        time.sleep(3)\n",
    "        source = driver.page_source\n",
    "        cla = bs(source, \"html.parser\").find_all(\"button\")[[x.text for x in bs(source, \"html.parser\").find_all(\"button\")].index(\"Поколение\")].get(\"class\")[0]\n",
    "        \n",
    "        button = driver.find_element(By.CLASS_NAME, cla)\n",
    "        button.click()\n",
    "\n",
    "        source = driver.page_source\n",
    "        # driver.quit()\n",
    "\n",
    "        tree = html.fromstring(source)\n",
    "        div = tree.xpath('//div[@data-ga-stats-name=\"generation_card\"]')\n",
    "        labels = [x.get('aria-label').split(\", \") for x in div]\n",
    "        mas = []\n",
    "        for L in labels:\n",
    "            if len(L) == 1:\n",
    "                mas.append((0, L[0][0]))\n",
    "            else:\n",
    "                if isNumber(L[1][0]):\n",
    "                    mas.append((L[1][0], L[0][0]))\n",
    "                else:\n",
    "                    mas.append((1, L[0][0]))\n",
    "        gen_sty = mas\n",
    "        for gen_sty in mas:\n",
    "            generation = int(gen_sty[1])\n",
    "            restyling = int(gen_sty[0])\n",
    "            adding_url = car_url + f\"generation{generation}/restyling{restyling}\"\n",
    "            all_urls.append(adding_url)\n",
    "    else:\n",
    "        mas = [(i, mas[i]) for i in range(len(mas)) if mas[i] > 0]\n",
    "        if len(mas) == 0:\n",
    "            mas = [(0, 1)]\n",
    "\n",
    "        for gen_sty in mas:\n",
    "            generation = int(gen_sty[0])\n",
    "            max_sty = int(gen_sty[1])\n",
    "            for restyling in range(max_sty):\n",
    "                adding_url = car_url + f\"generation{generation}/restyling{restyling}\"\n",
    "                amo = await adding_amount(adding_url) # опять проверка корректности\n",
    "                if amo < 10**8: all_urls.append(adding_url)\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "### Функция-экстрактор всех лет, когда производилась конкретная\n",
    "### модель в конкретном поколоении и с конкретным рестайлингом\n",
    "async def getting_spec_years_for_model(model_url, driver = None):\n",
    "    if driver != None:\n",
    "        driver.get(model_url)\n",
    "        page_source = driver.page_source\n",
    "        tree = html.fromstring(page_source)\n",
    "    else:\n",
    "        response = requests.get(model_url, proxies= proxies)\n",
    "        tree = html.fromstring(response.content)\n",
    "    exact_name = tree.xpath('//button[@data-ftid=\"component_select_button\"]')[0]\n",
    "    years = exact_name.text_content().split(\", \")[1].replace(\"н.в.\", \"2024\").split(\" - \")\n",
    "    years = [int(x.strip()) for x in years]\n",
    "    all_urls = []\n",
    "    for i in range(years[0], years[1] + 1):\n",
    "        adding_url = model_url + f\"/year-{i}\"\n",
    "        amo = await adding_amount(adding_url) # проверка корректности\n",
    "        if amo < 10**8: all_urls.append(adding_url)\n",
    "    \n",
    "    return all_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [12, 1, 7, 2, 9, 4, 16, 3, 14, 13, 10, 8, 6, 11, 5, 15]\n",
    "\n",
    "### Команда-воркер для добавления цвета\n",
    "### запрашиваемого авто\n",
    "async def color_w(url_queue, session, results, pbar):\n",
    "    while True:\n",
    "        url = await url_queue.get()\n",
    "        if url is None:\n",
    "            break\n",
    "        try:\n",
    "            content = await fetch(session, url)\n",
    "            if content:\n",
    "                tree = html.fromstring(content)\n",
    "                amo = 0\n",
    "                if len(tree.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')) == 0:\n",
    "                    amo = 10**8\n",
    "                else:\n",
    "                    amo = html.tostring(tree.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')[0]).decode(\"utf-8\")\n",
    "                    amo = amo[amo.find(\">\") + 1:].split()[0].replace(\"&#160;\", \"\")\n",
    "                    amo = int(amo)\n",
    "            \n",
    "                if amo > 400:\n",
    "                    if \"order_d\" in url:\n",
    "                        st = url[:url.find(\"?\")]\n",
    "                        U = [st + \"/\"*int(st[-1] != \"/\") + f\"?colorid[]={i}&order_d=asc#tabs\" for i in colors]\n",
    "                    else:\n",
    "                        U = [url + \"/\"*int(url[-1] != \"/\") + f\"?colorid[]={i}\" for i in colors]\n",
    "                    results.append(U)\n",
    "                else:\n",
    "                    results.append([url])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "        finally:\n",
    "            url_queue.task_done()\n",
    "            pbar.update(1)\n",
    "\n",
    "### Сама асинхронная команда\n",
    "async def color_sratification(urls):\n",
    "    url_queue = asyncio.Queue()\n",
    "    results = []\n",
    "\n",
    "    for url in urls:\n",
    "        url_queue.put_nowait(url)\n",
    "\n",
    "    connector = ProxyConnector.from_url(TOR_PROXY)\n",
    "    async with ClientSession(connector=connector) as session:\n",
    "        with tqdm(total=len(urls), desc=f\"Добавляем стратификацию по цвету\") as pbar:\n",
    "            workers = [asyncio.create_task(color_w(url_queue, session, results, pbar)) for _ in range(REQUEST_AMOUNT)]\n",
    "            \n",
    "            await url_queue.join()\n",
    "\n",
    "            for _ in range(REQUEST_AMOUNT):\n",
    "                await url_queue.put(None)\n",
    "\n",
    "            await asyncio.gather(*workers)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Команда-воркер для подсчета количества объявлений по конкретной\n",
    "### ссылке. Является частью бэк-дора сайта: мы сначала считаем количество\n",
    "### объявлений по ссылке, а потом начинаем стратифицировать объявления\n",
    "### по пробегу, указанному в объявлении (сайт опрометчиво дает возможность\n",
    "### указывать любой пробег с точностью до километра)\n",
    "async def append_amo(url_queue, session, results, pbar):\n",
    "    while True:\n",
    "        url = await url_queue.get()\n",
    "        if url is None:\n",
    "            break\n",
    "        try:\n",
    "            content = await fetch(session, url)\n",
    "            if content:\n",
    "                tree = html.fromstring(content)\n",
    "                amo = 0\n",
    "                if len(tree.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')) == 0:\n",
    "                    amo = 10**8\n",
    "                else:\n",
    "                    amo = html.tostring(tree.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')[0]).decode(\"utf-8\")\n",
    "                    amo = amo[amo.find(\">\") + 1:].split()[0].replace(\"&#160;\", \"\")\n",
    "                    amo = int(amo)\n",
    "            \n",
    "                results.append((url, amo))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "        finally:\n",
    "            url_queue.task_done()\n",
    "            pbar.update(1)\n",
    "\n",
    "### Сама асинхронная команда\n",
    "async def append_amo_for_url_list(urls):\n",
    "    url_queue = asyncio.Queue()\n",
    "    results = []\n",
    "\n",
    "    for url in urls:\n",
    "        url_queue.put_nowait(url)\n",
    "\n",
    "    connector = ProxyConnector.from_url(TOR_PROXY)\n",
    "    async with ClientSession(connector=connector) as session:\n",
    "        with tqdm(total=len(urls), desc=f\"Подсчет количества ссылок по запросу\") as pbar:\n",
    "            workers = [asyncio.create_task(append_amo(url_queue, session, results, pbar)) for _ in range(REQUEST_AMOUNT)]\n",
    "            \n",
    "            await url_queue.join()\n",
    "\n",
    "            for _ in range(REQUEST_AMOUNT):\n",
    "                await url_queue.put(None)\n",
    "\n",
    "            await asyncio.gather(*workers)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Команда-воркер для разделения объявлений по указанному\n",
    "### в них пробегу. Де-факто, это итерация бин-поиска по пробегу\n",
    "async def stratifier(url_queue, session, results, pbar):\n",
    "    while True:\n",
    "        mas = await url_queue.get()\n",
    "        if type(mas) == type(None):\n",
    "            break\n",
    "        url, amo = mas\n",
    "        try:\n",
    "\n",
    "            if url == url.replace(\"probeg\", \"\"):\n",
    "                x = (url, 0, 1500000, amo)\n",
    "            else:\n",
    "                extract = url[url.find(\"probeg\") - 3:]\n",
    "                extract = extract.split(\"&\")\n",
    "                probeg = [int(x[x.find(\"=\") + 1:]) for x in extract]\n",
    "\n",
    "                if \"color\" in url:\n",
    "                    if \"&min\" in url:\n",
    "                        url_no_probeg = url[:url.find(\"&min\")]\n",
    "                    else:\n",
    "                        url_no_probeg = url[:url.find(\"&max\")]\n",
    "                else:\n",
    "                    if \"/?min\" in url:\n",
    "                        url_no_probeg = url[:url.find(\"/?min\")]\n",
    "                    else:\n",
    "                        url_no_probeg = url[:url.find(\"/?max\")]\n",
    "\n",
    "                if len(probeg) == 1:\n",
    "                    if url != url.replace(\"minprobeg\", \"\"):\n",
    "                        x = (url_no_probeg, probeg[0], 1500000, amo)\n",
    "                    if url != url.replace(\"maxprobeg\", \"\"):\n",
    "                        x = (url_no_probeg, 0, probeg[0], amo)\n",
    "                else:\n",
    "                    x = (url_no_probeg, probeg[0], probeg[1], amo)\n",
    "\n",
    "            new_folds = []\n",
    "            av_probeg = int((x[1] + x[2])/2)\n",
    "\n",
    "            if \"?\" in x[0]:\n",
    "                mod_url1 = x[0] + f\"&minprobeg={x[1]}&maxprobeg={av_probeg}\"\n",
    "            else:\n",
    "                mod_url1 = x[0] + f\"/?minprobeg={x[1]}&maxprobeg={av_probeg}\"\n",
    "            \n",
    "            content1 = await fetch(session, mod_url1)\n",
    "            tree1 = html.fromstring(content1)\n",
    "            amo1 = 0\n",
    "            if len(tree1.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')) == 0:\n",
    "                amo1 = 10**8\n",
    "            else:\n",
    "                amo1 = html.tostring(tree1.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')[0]).decode(\"utf-8\")\n",
    "                amo1 = amo1[amo1.find(\">\") + 1:].split()[0].replace(\"&#160;\", \"\")\n",
    "                amo1 = int(amo1)\n",
    "\n",
    "            if amo1 < 10**8:\n",
    "                f1 = (mod_url1, x[1], max(av_probeg - 1, x[1]), amo1)\n",
    "                if f1[1] == f1[2] and amo1 > 800:\n",
    "                    f1[-1] = 10**8\n",
    "                new_folds.append(f1)\n",
    "            \n",
    "            if \"?\" in x[0]:\n",
    "                mod_url2 = x[0] + f\"&minprobeg={av_probeg}&maxprobeg={x[2]}\"\n",
    "            else:\n",
    "                mod_url2 = x[0] + f\"/?minprobeg={av_probeg}&maxprobeg={x[2]}\"\n",
    "\n",
    "            content2 = await fetch(session, mod_url2)\n",
    "            tree2 = html.fromstring(content2)\n",
    "            amo2 = 0\n",
    "            if len(tree2.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')) == 0:\n",
    "                amo2 = 10**8\n",
    "            else:\n",
    "                amo2 = html.tostring(tree2.xpath('//div[@class=\"css-1xkq48l eckkbc90\"]')[0]).decode(\"utf-8\")\n",
    "                amo2 = amo2[amo2.find(\">\") + 1:].split()[0].replace(\"&#160;\", \"\")\n",
    "                amo2 = int(amo2)\n",
    "\n",
    "            if amo2 < 10**8:\n",
    "                f2 = (mod_url2, av_probeg, x[2], amo2)\n",
    "                if f2[1] == f2[2] and amo2 > 800:\n",
    "                    f2[-1] = 10**8\n",
    "                new_folds.append(f2)\n",
    "\n",
    "            appending = [(x[0], x[-1]) for x in new_folds]\n",
    "\n",
    "            for y in appending:\n",
    "                results.append(y)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "        finally:\n",
    "            url_queue.task_done()\n",
    "            pbar.update(1)\n",
    "\n",
    "### Сама асинхронная команда\n",
    "async def first_iter_of_bin_search(urls_n_amos):\n",
    "    url_queue = asyncio.Queue()\n",
    "    results = []\n",
    "\n",
    "    for mas in urls_n_amos:\n",
    "        url_queue.put_nowait(mas)\n",
    "\n",
    "    connector = ProxyConnector.from_url(TOR_PROXY)\n",
    "    async with ClientSession(connector=connector) as session:\n",
    "        with tqdm(total=len(urls_n_amos), desc=f\"Стратифицируем ссылки по пробегу\") as pbar:\n",
    "            workers = [asyncio.create_task(stratifier(url_queue, session, results, pbar)) for _ in range(REQUEST_AMOUNT)]\n",
    "            \n",
    "            await url_queue.join()\n",
    "\n",
    "            for _ in range(REQUEST_AMOUNT):\n",
    "                await url_queue.put(None)\n",
    "\n",
    "            await asyncio.gather(*workers)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Пример марок для парсинга\n",
    "cars = \"\"\"bentley\n",
    "rolls-royce\n",
    "Chery\n",
    "Geely\n",
    "Honda\n",
    "Land Rover\n",
    "Lexus\"\"\"\n",
    "\n",
    "cars = cars.split(\"\\n\")\n",
    "ALL_CARS = [x.lower().strip().replace(\" \", \"_\") for x in cars] # приводим \n",
    "\n",
    "\n",
    "all_cars = []\n",
    "for car_name in tqdm(ALL_CARS, \"Оценка количества объявлений для машин\"):\n",
    "    base_url = f'https://auto.drom.ru/archive/{car_name}/all/'\n",
    "    amo = await adding_amount(base_url)\n",
    "    all_cars.append([car_name, amo])\n",
    "all_cars = sorted(all_cars, key = lambda x: x[1])\n",
    "\n",
    "all_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_start_with = 0\n",
    "\n",
    "for j, car_name in enumerate(all_cars[to_start_with:]):\n",
    "\n",
    "    to_start_with = j\n",
    "\n",
    "    start_time = time.time()\n",
    "    car_name, amo = car_name[0], car_name[1]\n",
    "    print(f\"Номер в очереди: {j + 1} из {len(all_cars)}. Парсинг {car_name}. Всего объявлений {amo}\")\n",
    "    base_url = f'https://auto.drom.ru/archive/{car_name}/all/'\n",
    "    valid_urls = []\n",
    "    all_models = []\n",
    "    amo = await adding_amount(base_url)\n",
    "    if amo <= 400:\n",
    "        valid_urls.append(base_url)\n",
    "    else:\n",
    "        all_models_url = getting_models_for_car(car_name)\n",
    "        all_models = await extracting_models_url(all_models_url)\n",
    "    \n",
    "    PROXY = \"socks5://127.0.0.1:9150\"\n",
    "    driver_service = Service(ChromeDriverManager().install())\n",
    "    chrome_path = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.binary_location = chrome_path\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(f'--proxy-server={PROXY}')\n",
    "    driver = webdriver.Chrome(service=driver_service, options=chrome_options)\n",
    "\n",
    "    for model in tqdm(all_models, desc=f\"Собираем уникальные категории каждой модели {car_name}\"):\n",
    "        amo = await adding_amount(model)\n",
    "        if amo <= 400:\n",
    "            valid_urls = valid_urls + [model]\n",
    "        else:\n",
    "            extra_list = await getting_spec_models_for_car(model, driver)\n",
    "            for spec_model in extra_list:\n",
    "                amo = await adding_amount(spec_model)\n",
    "                if amo <= 400:\n",
    "                    valid_urls = valid_urls + [spec_model]\n",
    "                else:\n",
    "                    try:\n",
    "                        ex = await getting_spec_years_for_model(spec_model, driver=driver)\n",
    "                    except (ParseError, ParserError, IndexError):\n",
    "                        try:\n",
    "                            ex = await getting_spec_years_for_model(spec_model, driver=driver)\n",
    "                        except (ParseError, ParserError, IndexError):\n",
    "                            # print(\"3\")\n",
    "                            time.sleep(10)\n",
    "                            ex = await getting_spec_years_for_model(spec_model, driver=driver)\n",
    "                    valid_urls = valid_urls + ex\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    href = pd.DataFrame({\"Ссылка\": valid_urls})\n",
    "    href.to_excel(f\"./cars/{car_name}_href.xlsx\", index = False)\n",
    "\n",
    "    print(\"=\"*50, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for filename in os.listdir(r\"./cars/\"):\n",
    "    if filename.endswith(\"href.xlsx\"):\n",
    "        df = pd.read_excel(f\"./cars/{filename}\")\n",
    "        files = files + list(df[\"Ссылка\"])\n",
    "\n",
    "print(\"Всего ссылок для дальнейшего сбора объявлений:\", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_files = await color_sratification(files)\n",
    "b = new_files[0]\n",
    "for i in range(1, len(new_files)):\n",
    "    b = b + new_files[i]\n",
    "new_files = b\n",
    "\n",
    "print(\"Всего ссылок для дальнейшего сбора объявлений (+ разный цвет):\",len(new_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Бэк-ап на всякий случай\n",
    "href = pd.DataFrame({\"Ссылка\": new_files})\n",
    "list_car_name = \"_\".join([x[0] for x in all_cars])\n",
    "href.to_excel(f\"./cars/{list_car_name}_href.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = pd.read_excel(f\"./cars/{list_car_name}_href.xlsx\")\n",
    "new_files = list(href[\"Ссылка\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_n_amo = await append_amo_for_url_list(new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Бэк-ап на всякий случай\n",
    "href = pd.DataFrame({\"Ссылка\": [x[0] for x in url_n_amo], \"Кол-во\": [x[1] for x in url_n_amo]})\n",
    "href.to_excel(f\"./cars/{list_car_name}_href.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = pd.read_excel(f\"./cars/{list_car_name}_href.xlsx\")\n",
    "href = href.to_dict(\"list\")\n",
    "url_n_amo = [(href[\"Ссылка\"][i], href[\"Кол-во\"][i]) for i in range(len(href[\"Ссылка\"]))]\n",
    "new_files = [url_n_amo[0]]\n",
    "\n",
    "for i in range(1, len(url_n_amo)):\n",
    "    if url_n_amo[i][1] < 10**8:\n",
    "        new_files.append(url_n_amo[i])\n",
    "\n",
    "final = []\n",
    "remain = []\n",
    "\n",
    "for i in range(len(new_files)):\n",
    "    x = new_files[i]\n",
    "    if x[1] <= 400:\n",
    "        final.append(x)\n",
    "    elif x[1] <= 800:\n",
    "        final.append((x[0], x[1]//2))\n",
    "        inverse = x[0] + \"/?\"*int(x[0].find(\"?\") == -1) + \"&\"*int(x[0].find(\"?\") != -1) + \"order_d=asc#tabs\"\n",
    "        final.append((inverse, x[1]//2))\n",
    "    else:\n",
    "        remain.append(x)\n",
    "\n",
    "print(\"Готовых ссылок:\", len(final))\n",
    "print(\"Ссылок для доработки:\", len(remain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = []\n",
    "### Фактический, бинарный поиск\n",
    "while len(remain) > 10:\n",
    "    print(\"Готовых ссылок:\", len(final))\n",
    "    print(\"Ссылок для доработки:\", len(remain))\n",
    "    print(\"Ссылок с критической проблемой:\", len(problems))\n",
    "    n_f = await first_iter_of_bin_search(remain)\n",
    "    remain = []\n",
    "    for i in range(len(n_f)):\n",
    "        x = n_f[i]\n",
    "        if x[1] <= 400:\n",
    "            final.append(x)\n",
    "        elif x[1] <= 800:\n",
    "            final.append((x[0], x[1]//2))\n",
    "            inverse = x[0] + \"/?\"*int(x[0].find(\"?\") == -1) + \"&\"*int(x[0].find(\"?\") != -1) + \"order_d=asc#tabs\"\n",
    "            final.append((inverse, x[1]//2))\n",
    "        elif x[1] == 10**8:\n",
    "            problems.append(x)\n",
    "        else:\n",
    "            remain.append(x)\n",
    "    print(\"=\"*50, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Отсмотр глазами \"неудачных\" ссылок, которые нужно доделить руками.\n",
    "### Христоматийный пример: Лад Приор первого поколения 2008 года производства черного цвета\n",
    "### с пробегом 200 тысяч км (https://auto.drom.ru/archive/lada/priora/generation1/restyling0/year-2008/?colorid[]=1&minprobeg=200000&maxprobeg=200000)\n",
    "### больше 800 (854 на 29.01.2025) => наблюдения \"посередине\" никак не достатать,\n",
    "### отчего код сверху зациклится до бесконечности, если будет \"while True\" в начале\n",
    "remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Пример доработки руками!!!\n",
    "\n",
    "# n_f = []\n",
    "\n",
    "# n_f.append(\n",
    "#     (\"https://auto.drom.ru/archive/honda/accord/generation8/restyling0/year-2008/?colorid[]=1&minprobeg=164062&maxprobeg=180000\",\n",
    "#      629)\n",
    "# )\n",
    "\n",
    "# n_f.append(\n",
    "#     (\"https://auto.drom.ru/archive/honda/accord/generation8/restyling0/year-2008/?colorid[]=1&minprobeg=180000&maxprobeg=187500\",\n",
    "#      179)\n",
    "# )\n",
    "\n",
    "# n_f.append(\n",
    "#     (\"https://auto.drom.ru/archive/honda/accord/generation8/restyling0/year-2008/?colorid[]=1&minprobeg=187500&maxprobeg=200000\",\n",
    "#      529)\n",
    "# )\n",
    "\n",
    "# n_f.append(\n",
    "#     (\"https://auto.drom.ru/archive/honda/accord/generation8/restyling0/year-2008/?colorid[]=1&minprobeg=200001&maxprobeg=210937\",\n",
    "#      278)\n",
    "# )\n",
    "\n",
    "# n_f.append(\n",
    "#     (\"https://auto.drom.ru/archive/geely/monjaro/generation1/restyling0/year-2023/new/?colorid[]=3\",\n",
    "#      565)\n",
    "# )\n",
    "\n",
    "# n_f.append(\n",
    "#     (\"https://auto.drom.ru/archive/geely/monjaro/generation1/restyling0/year-2023/used/?colorid[]=3&maxprobeg=23437\",\n",
    "#      387)\n",
    "# )\n",
    "\n",
    "### Самый плохой случай, где мы потеряли 54 наблюдения...\n",
    "# n_f.append(\n",
    "#     (\"https://auto.drom.ru/archive/lada/priora/generation1/restyling0/year-2008/?colorid[]=1&minprobeg=200000&maxprobeg=200000\",\n",
    "#      800)\n",
    "# )\n",
    "\n",
    "# for i in range(len(n_f)):\n",
    "#     x = n_f[i]\n",
    "#     if x[1] <= 400:\n",
    "#         final.append(x)\n",
    "#     elif x[1] <= 800:\n",
    "#         final.append((x[0], x[1]//2))\n",
    "#         inverse = x[0] + \"/?\"*int(x[0].find(\"?\") == -1) + \"&\"*int(x[0].find(\"?\") != -1) + \"order_d=asc#tabs\"\n",
    "#         final.append((inverse, x[1]//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Всего объявлений для парсинга:\", sum([x[1] for x in final]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Пример разделения ссылок для первичного парсинга\n",
    "\n",
    "i = 0\n",
    "new_files = [x[0] for x in final]\n",
    "while i < len(new_files):\n",
    "    ongoing = new_files[i:min(i + 37, len(new_files))]\n",
    "    pd.DataFrame({\"Ссылка\": ongoing}).to_excel(f\"./cars/href/{i//37}car.xlsx\", index = False)\n",
    "    i += 37"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_comp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
