## Описание проекта

**Последнее обновление проекта: 29.01.2025.**

Данный парсер предназначен для сбора информации по архивным объявлениям, представленных на сайте <a href="https:/auto.drom.ru/archive/" target="_blank"> Дром.ру </a> - крупнейшей в России онлайн-платформы, агрегирующую предложение на вторичном рынке автомобилей, через которую проходит до 60% соответстующего трафика. Собранный датасет также размещен в соответствующей папке  проекта (если ещё не, то он скоро обязательно появится).

## Структура проекта и некоторые комментарии

- Предполагается, что для работы на своем устройстве пользователь загрузит из проекта всю папку root на Рабочий стол (если использует ОС Windows; если нет, то тут автор бессилен).
- Все архивы и ноутбук zakupki.ipynb представляют из себя образец данных, собранных автором, оригинального кода, их обработавшего, и выходных данных и используются для демонстрации результатов его работы. Код, приспособленный для форматирования - это ноутбук zakupki_for_scaling.ipynb. Чтобы работать в нем необходимо изменить DESKTOP_USER_NAME на имя пользователя на своем копьютере (в разделе "Создание таблиц с нужными данными" в самой первой вкладке "Функции" в первой ячейке).
- Все ячейки кода необходимо запускать последовательно или несколько раз (об этом подробнее ниже).
- На компьютере необходимо иметь браузер Хром и скаченное в нем расширение EditThisCookie (<a href="https://chromewebstore.google.com/detail/editthiscookie/fngmhnnpilhplaeedifhccceomclgfbg?pli=1" target="_blank">ссылка</a>).
- Формальное описание всей процедуры сбора данных есть во втором разделе курсовой работы, оставленной выше.

## Алгоритм работы с парсером

1. Рассмотрим некоторое решение ФАСа со списком фирм, означенных как картель. В данном коде рассмотрено дело № 4-14.32-402/00-30-17 (но он способен работать и с другими). Из дела нужно взять помимо этого коды ОКДП (если есть; иначе подбирать самому, как в примере), даты, во время которых реализовывалось соглашение, и список конкурентных фирм, участвовавших в закупках вместе с фирмами-участниками картеля (если нет, то рассмотреть торги без фирм-участников картеля и верить в то, что другие все конкурентные). В целом, если нет нужды в сравнительном анализе, то конкурентные фирмы не нужны.

2. Необходимо найти ИНН всех фирм. Можно это делать вручную или с использованием API СПАРКа (такое вроде есть, но там нужен корпоративный доступ). Результаты этой процедуры лежат в carteling.txt и competing.txt .

3. На <a href="https://zakupki.gov.ru/" target="_blank">ЕИС</a> ищем закупки с нужными параметрами (ФЗ, дата, площадка: Сбербанк-АСТ и т.д.) и вставляем ИНН фирм из картеля из пункта (2). Дальше выгружаем всю информацию в xlsx формате, если закупок за дату больше 5000, то дробим по дате так, чтобы скачать в несколько xlsx всё в итоге. Файлы называем числами, начиная с 1. Все эти файлы сохраняем в папку to_parse.

4. Теперь переходим к коду. Проигрываем ячейки в "Библиотеки", "Функции", последовательно воспроизводим ячейки из "Общие таблицы с выгрузкой | Картель". 

5. Далее открываем окно в Хроме и копируем cookie файлы со страницы любой закупки на сайте Сбербанк-АСТ (например, <a href="https://www.sberbank-ast.ru/ViewDocument.aspx?id=299236046" target="_blank">здесь</a>). Копировать следует в ячейку кода в разделе "Таблицы с участниками | Картель", которая начинается, как content = ... . Вставить скопированный текст необходимо между """ """, как показано в примере. После этого нужно последовательно проиграть все ячейки в этом же разделе. Самая важная и долгая из них - та, перед которой написан комментарий.

6. Разделы кода, названные в конце "... | Конкурентные", устроены так же и работают аналогично.

7. Дополнительно в оригинальном коде показана методика расчета показателей, характеризующих закупочные процедуры, из статей Huber and Imhof (2023, 2019), Huber, Imhof and Ishii (2022), Imhof (2019), Imhof, Karag¨ok and Rutz (2018), Wallimann, Imhof and Huber (2023).

## Что можно улучшить?

К сожалению, работа программы в некоторых местах предполагает ручные действия. На данный момент таковыми являются:

- Определение изначального пула автомобилей для парсинга
- Ручная до-обработка некоторых ссылок на этапе сбора (о чем подробнее в коде на примере)
- Код не работает в случае, если на этапе первичного парсинга возникают ссылки на спецтехнику (пример: Volvo, долго с ним возился)
- Создание папок в репозитории для корректной систематизации спаршенных файлов
- Отсутствие инструментария для копирования репозитория (я не знаю git)

Также отдельно стоит отметить bottle-neck в разделе первичного сбора ссылок: работа с эмулятором Гугл-хром через selenium не удовлетворяет требованиям к скорости (хотя и качественно справляется с поставленной задачей). Возможное решение проблемы: перепись под более быстрые библиотеки (например, Playwright).
